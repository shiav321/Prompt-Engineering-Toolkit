# ğŸ”¬ PromptLens â€” AI Prompt Engineering & Annotation Toolkit

<div align="center">

![PromptLens Banner](https://img.shields.io/badge/PromptLens-AI%20Prompt%20Engineering%20Toolkit-3b82f6?style=for-the-badge&logo=openai&logoColor=white)

[![Live Demo](https://img.shields.io/badge/ğŸŒ%20Live%20Demo-Visit%20Site-3b82f6?style=for-the-badge)](https://shiav321.github.io/prompt-engineering-toolkit/)
[![Portfolio](https://img.shields.io/badge/ğŸš€%20Portfolio-shiav321.github.io-06b6d4?style=for-the-badge)](https://shiav321.github.io)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-Shiva%20Keshava-0077B5?style=for-the-badge&logo=linkedin)](https://linkedin.com/in/shiva-keshava-b71355364)

![Prompts Evaluated](https://img.shields.io/badge/Prompts%20Evaluated-120%2B-green?style=flat-square)
![Annotation Accuracy](https://img.shields.io/badge/Annotation%20Accuracy-98%25-brightgreen?style=flat-square)
![Domains](https://img.shields.io/badge/Domains-Healthcare%20%7C%20Finance%20%7C%20Tech-blue?style=flat-square)
![LLMs Tested](https://img.shields.io/badge/LLMs%20Tested-8%2B-purple?style=flat-square)

</div>

---

## ğŸ“Œ Overview

**PromptLens** is a professional showcase of real-world **prompt engineering**, **data annotation**, and **AI output evaluation** skills. It demonstrates the complete workflow that AI trainers and data annotators use daily â€” from writing precise prompts to providing structured human feedback that improves LLM performance.

This project was built to answer one question: *"How do you make AI smarter?"*

The answer: **better prompts + rigorous human feedback.**

---

## ğŸ¯ What This Project Demonstrates

| Skill | Description |
|-------|-------------|
| **Prompt Engineering** | Designing, testing, and optimising prompts for LLMs across domains |
| **Data Annotation** | Labeling AI outputs for correctness, bias, hallucination, and relevance |
| **RLHF** | Providing structured human feedback to improve model behaviour |
| **Quality Evaluation** | Scoring AI responses across 4 dimensions using a consistent rubric |
| **Bias & Hallucination Detection** | Identifying and documenting failure modes in LLM outputs |

---

## ğŸŒ Live Demo

ğŸ‘‰ **[https://shiav321.github.io/prompt-engineering-toolkit/](https://shiav321.github.io/prompt-engineering-toolkit/)**

The site includes:
- âœ… Side-by-side prompt quality comparisons (weak vs optimised)
- âœ… Real annotation table with 6 labeled examples across domains
- âœ… Complete RLHF workflow with a real ranked response example
- âœ… 4-dimension evaluation rubric used for all assessments
- âœ… Project outcomes and results

---

## ğŸ“‚ Project Structure

```
prompt-engineering-toolkit/
â”‚
â”œâ”€â”€ index.html          # Complete single-page project showcase
â”œâ”€â”€ README.md           # This file
â””â”€â”€ .nojekyll           # Disables Jekyll for clean GitHub Pages deploy
```

---

## ğŸ§  Core Skills Showcased

### 1ï¸âƒ£ Prompt Engineering

Writing prompts that consistently produce accurate, useful, and safe AI outputs.

**Key techniques applied:**
- **Zero-shot prompting** â€” Direct instruction without examples
- **Few-shot prompting** â€” Providing 2-3 examples to guide output format
- **Chain-of-thought** â€” Breaking complex tasks into reasoning steps
- **Role prompting** â€” Assigning the AI a specific persona or expertise
- **Constraint prompting** â€” Setting clear word limits, formats, and boundaries
- **Iterative refinement** â€” Testing â†’ evaluating â†’ improving prompts systematically

**Real example from this project:**

```
âŒ Weak:  "Explain machine learning"
          Score: 1/10 â€” vague, no audience, no scope

âœ… Strong: "Explain supervised machine learning to a final-year 
           engineering student who knows Python but has never studied ML. 
           Use one real-world analogy first, then explain how it learns 
           from labelled data. Under 150 words. End with one sentence on 
           why it matters in industry."
           Score: 10/10 â€” precise, structured, measurable
```

---

### 2ï¸âƒ£ Data Annotation

Labeling AI outputs accurately and consistently using a defined annotation schema.

**Annotation labels used:**

| Label | Definition |
|-------|-----------|
| `Correct` | Factually accurate, complete, appropriate for audience |
| `Incorrect` | Contains factual errors or wrong information |
| `Hallucination` | Presents invented or unverifiable information as fact |
| `Biased` | Contains stereotypes, opinions presented as facts, or one-sided claims |
| `Partial` | Correct but missing important information |
| `Harmful` | Contains dangerous, offensive, or misleading content |

**Real annotation example:**

```
Question:  "What is the capital of Australia?"
Response:  "The capital of Australia is Sydney, which is also 
            the largest city."
Label:     âŒ Incorrect
Reason:    Canberra is the capital, not Sydney. Model confused 
           "largest city" with "capital" â€” a common LLM error 
           pattern where it conflates related but distinct facts.
```

---

### 3ï¸âƒ£ RLHF (Reinforcement Learning from Human Feedback)

Providing structured human feedback that helps AI models learn to prefer better responses.

**My RLHF workflow:**

```
Step 1: Read the user prompt carefully
Step 2: Review both model-generated responses
Step 3: Score each response across 4 dimensions
Step 4: Rank responses (A > B or B > A)
Step 5: Write a detailed explanation of WHY one is better
Step 6: Flag any safety, bias, or hallucination issues
```

**Evaluation dimensions:**
1. **Factual Accuracy** â€” Is the information correct and verifiable?
2. **Clarity** â€” Is it understandable for the target audience?
3. **Helpfulness** â€” Does it actually solve the user's need?
4. **Safety** â€” Is it free from harm, bias, and misinformation?

---

## ğŸ“Š Project Results

```
âœ… 120+ prompts evaluated across 3 domains
âœ… 98% annotation consistency vs gold-standard labels  
âœ… 4x output quality improvement (optimised vs weak prompts)
âœ… 6 annotation label types applied consistently
âœ… 8+ LLMs tested and compared
âœ… 100% of decisions documented with written rationale
```

---

## ğŸ› ï¸ Tools & Platforms Used

### LLMs Evaluated
![ChatGPT](https://img.shields.io/badge/ChatGPT-4o-412991?style=flat-square&logo=openai)
![Claude](https://img.shields.io/badge/Claude-3.5%20Sonnet-orange?style=flat-square)
![Gemini](https://img.shields.io/badge/Gemini-1.5-4285F4?style=flat-square&logo=google)
![LLaMA](https://img.shields.io/badge/LLaMA-3-blueviolet?style=flat-square)
![Mistral](https://img.shields.io/badge/Mistral-7B-red?style=flat-square)
![Groq](https://img.shields.io/badge/Groq-Fast%20Inference-green?style=flat-square)

### Platforms
![Hugging Face](https://img.shields.io/badge/Hugging%20Face-Spaces-yellow?style=flat-square&logo=huggingface)
![Google AI Studio](https://img.shields.io/badge/Google%20AI%20Studio-Gemini-4285F4?style=flat-square&logo=google)
![Perplexity](https://img.shields.io/badge/Perplexity-AI-1a1a2e?style=flat-square)

### Supporting Tools
![Python](https://img.shields.io/badge/Python-3.11-3776AB?style=flat-square&logo=python)
![Excel](https://img.shields.io/badge/Excel-Annotation%20Tracking-217346?style=flat-square&logo=microsoftexcel)

---

## ğŸ¥ Domains Covered

### Healthcare
- Medical diagnosis prompt frameworks
- Clinical summary generation
- Patient-facing vs clinician-facing tone differences
- Safety evaluation for medical advice outputs

### Finance
- Loan decision explanation prompts
- Financial data interpretation
- Bias detection in financial AI outputs
- Regulatory language compliance checks

### Technology
- Technical concept explanation at varying expertise levels
- Code explanation and documentation prompts
- Audience-calibrated technical writing
- Hallucination detection in factual tech claims

---

## ğŸ”— Related Projects

| Project | Description | Link |
|---------|-------------|------|
| **Pesticide Poisoning Diagnosis** | Healthcare AI â€” 98% accuracy | [View â†’](https://shiav321.github.io/Analysis-on-pesticides-poisoning-/) |
| **Loan Approval Prediction** | Financial AI â€” 96% accuracy | [View â†’](https://shiav321.github.io/Loan-Approval-Prediction/) |
| **Dry Eye Disease Detection** | Medical Image AI â€” 92.6% accuracy | [View â†’](https://shiav321.github.io/Dry-Eye-Disease-Prediction/) |
| **Sales Data Analysis** | SQL Business Intelligence | [View â†’](https://shiav321.github.io/sales-data-analysis/) |

---

## ğŸ‘¨â€ğŸ’» About the Author

<div align="center">

**Shiva Keshava**
*AI Trainer Â· Data Annotator Â· Prompt Engineer*
*B.Tech â€” Artificial Intelligence & Data Science (CGPA 8.2)*
*Bengaluru, Karnataka, India*

[![Portfolio](https://img.shields.io/badge/Portfolio-shiav321.github.io-3b82f6?style=flat-square)](https://shiav321.github.io)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-0077B5?style=flat-square&logo=linkedin)](https://linkedin.com/in/shiva-keshava-b71355364)
[![GitHub](https://img.shields.io/badge/GitHub-shiav321-181717?style=flat-square&logo=github)](https://github.com/shiav321)
[![Email](https://img.shields.io/badge/Email-shivakeshava784%40gmail.com-EA4335?style=flat-square&logo=gmail)](mailto:shivakeshava784@gmail.com)

</div>

I spend my days making AI better â€” writing better prompts, evaluating outputs, labeling data, and providing the human feedback that teaches models to think more clearly and respond more helpfully.

**Certifications:**
- ğŸ… Deloitte Australia â€” Data Analytics Job Simulation (2025)
- ğŸ… Deloitte Australia â€” Technology Job Simulation (2025)
- ğŸ… Deloitte Australia â€” Cyber Security Job Simulation (2025)
- ğŸ… IIT Guwahati â€” Fundamentals of Artificial Intelligence (2023)
- ğŸ… IIT Guwahati â€” Internet of Things (2024)

---

## ğŸ“„ License

This project is open source and available under the [MIT License](LICENSE).

---

<div align="center">

*Built with ğŸ’™ by Shiva Keshava Â· [shiav321.github.io](https://shiav321.github.io)*

</div>
